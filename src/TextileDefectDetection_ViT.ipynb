{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIzQdZHl_L9v",
        "outputId": "b014c6ba-d965-473c-e6f0-6713dbc3ef54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MbZgBNbFuv-o"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Key Settings\n",
        "DataDir = \"/content/drive/MyDrive/Colab Notebooks/FabricDefectDetection/Dataset\"\n",
        "BatchSize = 16\n",
        "ImageSize = 224\n",
        "TrainRatio = 0.8\n",
        "RandomSeed = 42\n",
        "torch.manual_seed(RandomSeed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ywl9AKOwobj",
        "outputId": "bcbd8f2c-7b53-48bf-f029-566142e87480"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78171a0bd8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To count images per class before splitting\n",
        "print(\"Images before splitting across class: \\n\")\n",
        "classes = sorted(os.listdir(DataDir))\n",
        "for cls in classes:\n",
        "  classpath = os.path.join(DataDir,cls)\n",
        "  count = len(os.listdir(classpath))\n",
        "  print(f\"{cls}:{count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIbojb0B1-Mb",
        "outputId": "fef2657f-fe21-44e7-ca07-2b1ab44aa96d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images before splitting across class: \n",
            "\n",
            "Broken stitch:112\n",
            "Needle mark:108\n",
            "Pinched fabric:108\n",
            "Vertical:101\n",
            "defect free:1666\n",
            "hole:281\n",
            "horizontal:136\n",
            "lines:157\n",
            "stain:398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Dataset and apply transformations\n",
        "train_transform = transforms.Compose( [\n",
        "    transforms.Resize((ImageSize, ImageSize)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness = 0.2, contrast = 0.2),\n",
        "    transforms.ToTensor()\n",
        "]\n",
        "\n",
        ")\n",
        "\n",
        "val_transform = transforms.Compose( [\n",
        "    transforms.Resize((ImageSize, ImageSize)),\n",
        "    transforms.ToTensor()\n",
        "\n",
        "]\n",
        ")\n",
        "\n",
        "FabricImages = datasets.ImageFolder(DataDir,transform = val_transform)"
      ],
      "metadata": {
        "id": "JYtgfTa26K0Z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified Split\n",
        "\n",
        "# Representing position of all images in the dataset\n",
        "indices = list(range(len(FabricImages)))\n",
        "\n",
        "#To get the labels\n",
        "labels = [FabricImages.imgs[i][1] for i in indices]\n",
        "\n",
        "#to perform stratified split\n",
        "train_id, val_id = train_test_split(\n",
        "    indices,\n",
        "    test_size = 0.2,\n",
        "    stratify = labels,\n",
        "    random_state = 42\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "k8okL_-n-0xs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To create train and validataion datasets\n",
        "#To take a subset of the original dataset (train and val) and apply specific transformation to it\n",
        "#Requirement: Training data with Augmentation and Validation data without Augmentation\n",
        "#Additonal: from torch.utils.data import Subset; its purpose is to select a subset of a dataset using list of indices\n",
        "\n",
        "class SubsetwithTransform(Subset):\n",
        "  def __init__(self,subset,transform):\n",
        "    # subset - a PyTorch subset object containing subset.dataset and subset.indices\n",
        "    super().__init__(subset.dataset, subset.indices)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img,label = super().__getitem__(idx)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img, label\n",
        "\n",
        "\n",
        "train_dataset = SubsetwithTransform(Subset(FabricImages,train_id),train_transform)\n",
        "val_dataset = SubsetwithTransform(Subset(FabricImages,val_id),val_transform)\n"
      ],
      "metadata": {
        "id": "h8Qnq6ceDMvU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset,batch_size = BatchSize, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset,batch_size = BatchSize, shuffle = False)"
      ],
      "metadata": {
        "id": "wQTTazuOKfNQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define The Model\n",
        "num_classes = len(FabricImages.classes)\n",
        "model = timm.create_model('vit_base_patch16_224',pretrained = True)\n",
        "model.head = nn.Linear(model.head.in_features, num_classes )\n",
        "\n",
        "#Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "#weight = class_weights; The loss will now penalize misclassification of minority classes more, helping the model learn them better\n",
        "\n",
        "#Specify the loss\n",
        "counts = [Counter(labels)[i] for i in range(num_classes)]\n",
        "total = sum(counts)\n",
        "weights = [total/c for c in counts]\n",
        "class_weights = torch.tensor(weights,dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "#specify the optimizer\n",
        "optimizer = optim.Adam(model.parameters(),lr=3e-4)\n",
        "\n"
      ],
      "metadata": {
        "id": "yJJ9EGorVEvh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  #put the model in training mode:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0 # to compute batch loss\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for images,labels in train_loader:\n",
        "    images,labels = images.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() *images.size(0)#To get the total loss of the batch by multiplying average loss of the batch with batch size\n",
        "    _,predicted_class = outputs.max(1)\n",
        "    total += labels.size(0) #to track of how many instances processed so far\n",
        "    correct += predicted_class.eq(labels).sum().item()\n",
        "\n",
        "  train_loss = running_loss / total\n",
        "  train_acc = (100 * correct) / total\n",
        "\n",
        "  #validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images,labels in val_loader:\n",
        "      images,labels = images.to(device),labels.to(device)\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item() * images.size(0)\n",
        "      _,predicted_class = outputs.max(1)\n",
        "      total +=labels.size(0)\n",
        "      correct += predicted_class.eq(labels).sum().item()\n",
        "\n",
        "  val_loss = val_loss / total\n",
        "  val_accuracy = (100*correct) / total\n",
        "\n",
        "  print(f\"Epoch:[{epoch+1}/{num_epochs}] |\"\n",
        "        f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}% |\",\n",
        "        f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}% \"\n",
        "        )\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"vit_fabric_defect.pth\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "59eXRRqVfCJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38daf7c0-252c-4990-ad0a-04bf1a6df358"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:[1/10] |Training Loss: 1.8155, Training Accuracy: 42.23% | Validation Loss: 1.5118, Validation Accuracy: 42.83% \n",
            "Epoch:[2/10] |Training Loss: 1.2472, Training Accuracy: 56.79% | Validation Loss: 0.9397, Validation Accuracy: 70.36% \n",
            "Epoch:[3/10] |Training Loss: 1.0449, Training Accuracy: 70.40% | Validation Loss: 0.8932, Validation Accuracy: 66.29% \n",
            "Epoch:[4/10] |Training Loss: 0.9783, Training Accuracy: 72.85% | Validation Loss: 0.9824, Validation Accuracy: 63.68% \n",
            "Epoch:[5/10] |Training Loss: 0.9671, Training Accuracy: 72.16% | Validation Loss: 0.9214, Validation Accuracy: 69.22% \n",
            "Epoch:[6/10] |Training Loss: 0.7288, Training Accuracy: 79.74% | Validation Loss: 0.6093, Validation Accuracy: 83.88% \n",
            "Epoch:[7/10] |Training Loss: 0.7494, Training Accuracy: 77.33% | Validation Loss: 0.9823, Validation Accuracy: 68.89% \n",
            "Epoch:[8/10] |Training Loss: 0.7505, Training Accuracy: 79.13% | Validation Loss: 0.6090, Validation Accuracy: 83.22% \n",
            "Epoch:[9/10] |Training Loss: 0.7247, Training Accuracy: 78.72% | Validation Loss: 0.5716, Validation Accuracy: 86.81% \n",
            "Epoch:[10/10] |Training Loss: 0.5792, Training Accuracy: 82.51% | Validation Loss: 0.9225, Validation Accuracy: 58.96% \n"
          ]
        }
      ]
    }
  ]
}